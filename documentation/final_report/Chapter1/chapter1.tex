\chapter{Background \& Objectives}

%This section should discuss your preparation for the project, including background reading, your analysis of the problem and the process or method you have followed to help structure your work.  It is likely that you will reuse part of your outline project specification, but at this point in the project you should have more to talk about. 
%
%\textbf{Note}: 
%
%\begin{itemize}
%   \item All of the sections and text in this example are for illustration purposes. The main Chapters are a good starting point, but the content and actual sections that you include are likely to be different.
%   
%   \item Look at the document on the Structure of the Final Report for additional guidance. 
%   
%\end {itemize}
%
%\section{Background}
%What was your background preparation for the project? What similar systems or research techniques did you assess? What was your motivation and interest in this project? 
%
%\section{Analysis}
%Taking into account the problem and what you learned from the background work, what was your analysis of the problem? How did your analysis help to decompose the problem into the main tasks that you would undertake? Were there alternative approaches? Why did you choose one approach compared to the alternatives? %%There should be a clear statement of the research questions, which you will evaluate at the end of the work. %%In most cases, the agreed objectives or requirements will be the result of a compromise between what would ideally have been produced and what was felt to be possible in the time available. A discussion of the process of arriving at the final list is usually appropriate.
%
%\section{Research Method}
%You need to describe briefly the life cycle model or research method that you used. You do not need to write about all of the different process models that you are aware of. Focus on the process model or research method that you have used. It is possible that you needed to adapt an existing method to suit your project; clearly identify what you used and how you adapted it for your needs.

\section{Background}

\subsection{Introduction}

The ability to decide between one or more alternate choices is a complex skill that as humans, we possess but rarely consciously observe. As human beings, we are expected to make decisions both large and small as part of everyday life, but . While most decisions will have typically little impact requiring almost unnoticeable levels of thought (e.g. \textit{``do I have peas or beans tonight with dinner?"}), others that could have potentially life-changing consequences (e.g. \textit{"do I take this new job or not?"}) will force an individual to consider much more closely.

For a human faced with making such decisions, identifying which out of a number of possible choices constitutes the ``right" decision is often regarded as a challenging and highly subjective task. %Each decision that a person makes will vary based upon their individual characteristics, which as a consequence can often result in contrasting decisions being taken in response to the same situation.% 
Before settling on a final decision, an individual will typically identify and evaluate all possible choices by considering each upon a weighting of merit \cite{rational-decision-model}. While variations in weighting between specific individuals can cause different decision outcomes, an important realisation comes from the understanding that all considerations made by an individual rely on their \textit{trust} in the accuracy and truth of information gathered during the identification and comparison of these alternative choices.

 In considering any kind of autonomous system that is expected to make unattended decisions, a crucial yet not always obvious aspect is a reliance upon this idea of \textit{trust}. In the same way as observed in humans, a system will typically depend upon underlying information from which it can assess that the final decision when made is most appropriate for a given situation.
 
 This information may be obtained from a variety of sources, ranging from raw sensor data up to the output of a sophisticated high-level algorithm. While the type and source of information may vary wildly from system-to-system, the underlying notion that at the time of making the final decision this information can, in one form or another, be \textit{trusted} to provide a reliable and accurate representation of the actual situation remains exactly the same. 
 
Given the importance of the relationship between reliable information and correct decision making, a major challenge within the field of autonomous robotics is maximising both the accuracy of input data, and the robustness of systems used to verify such accuracy.

  Accurate and efficient navigation plays a crucial role in allowing robots that require autonomous motion capabilities to make safe, yet objective decisions on how best to traverse from a starting position to a target position. This idea becomes evermore important when it is considered that a key use case for robotic systems typically involves the undertaking of tasks within environments deemed unfeasible or unsafe for humans to complete themselves. Observing examples of tasks where autonomous robots are expected to operate in particularly harsh environments, including bomb disposal \cite{bomb-disposal} and planetary exploration \cite{planet-explore}, it becomes clear that the ability to perform safe and robust navigation is vital to the survival of both mission and device.

As a problem-space, autonomous navigation can be decomposed into two key areas of focus; one of \textit{reactive} navigation, and the other of \textit{deliberative} navigation. Reactive, or local navigation is concerned with controlling the path of the robot through the immediate surrounding area, focussing in particular on the safe traversal around potentially dangerous terrain hazards including obstacles, steep slopes and precipices. In contrast, deliberative or global navigation is tasked with planning the ``high-level" path that a robot will follow in order to reach its destination. As a consequence, deliberative navigation tends to adopt a greater level of focus on calculating the most \textit{optimal} path through the environment over necessarily the one that is ``safest" for the robot. Traditionally within autonomous navigation systems, feedback from both the reactive and deliberative components is combined in order to arrive at final path that balances both safety and objectivity \cite{mer-rover}. 

A vital contributor to the success of reactive navigation and therefore to the overall survival of an autonomous robot is the identification and subsequent avoidance of obstacles. As a result, the development of accurate and robust obstacle detection systems is a keen and well-explored area of robotics research, with a variety of approaches now available adopting many types of sensor including sonar \cite{sonar-sensor} and laser scanning \cite{laser-sensor}. An alternative approach that has enjoyed increasingly greater research focus over the past decade is that of vision-based obstacle avoidance. This involves the analysis of images captured from one or more digital cameras in order to detect and classify possible dangers currently situated within the robot's field of view. 

Cameras are becoming an increasingly favourable sensor for use on robotic systems, due primarily to the impressive variety and quantity of potential data that can be captured \textit{simultaneously} from a single device. They are also one of few sensors that have experienced a consistent reduction in price over the past decade \cite{campbell}, making them viable for many types of project application and budget. 

\subsection{Computer Vision Principles}

\subsection{Related Work}

A large variety of solutions to vision-based navigation and obstacle avoidance have previously been published, with the majority proposing new or improved approaches to combining ``standard" computer-vision techniques including optical flow, feature tracking and template patch tracking with a variety of hardware configurations including monocular systems, stereo cameras and vision/3D hybrid systems. Typically, these solutions either strive to improve on previously published performance results, or to focus on the avoidance of specific types of obstacle (e.g. precipice or animal detection). 

%From investigating existing research conducted in the area of visual-based navigation and obstacle avoidance, it is possible to identify and compare potential benefits and challenges between various approaches that will in turn feed into future design decisions.

The first approach to be noted is that of the work of Campbell \textit{et al} \cite{campbell} in which the authors propose a system for estimating the change in position and rotation of a robot using information obtained solely via a single consumer-grade colour camera. The approach describes the use of feature-based tracking to estimate the optical flow-field between subsequent video frames, before taking these optical flow vectors currently corresponding to image coordinates, and back-projecting them onto a ``robot-centred" coordinate system to establish the incremental `real world' translation and rotation of the robot over a given time period. The detection and subsequent tracking of matching features between captured frames is provided via the \textit{OpenCV} library implementation of the Lucas and Kanade algorithm \cite{opencv-lucas-kanade-features}. 

Once gathered, these correspondences between features (i.e. optical flow vectors) are filtered to help remove outliers caused by matching error or occlusion. The criteria used for this filtering process focusses on verifying a smooth straight-line motion of same features between subsequent frames. It is reported that concentrating on the movement history of features across a subset of previous frames can provide a robust defence against outliers. This is because by definition, an outlier would typically be expected to be found outside of the projected straight-line trajectory of the mis-matched feature, thereby causing the motion behaviour of this feature to suddenly become erratic. Through the use of this filtering technique in conjunction with the correction of perspective distortion via calibration, the authors report they are able to allow for a wider, and potentially lower quality range of features to be initially detected in order to ensure adequate coverage of the entire image scene. 

In the final stage of the process proposed in \cite{campbell}, the current frame is sectioned into regions representing the ``sky" and ``ground" individually, from within which the observed optical flow vectors are used to calculate an estimation of the robot's incremental rotation and translation respectively.

%The authors report that this implementation provides a more efficient and robust version of the original algorithm proposed by Lucas and Kanade \cite{lucas-kanade-features}. Upon further investigation this improvement appears to be attributed to the proposal published by Bouguet \cite{j-bouguet} in which the author combines a multi-scale pyramidal representation of an image with the existing Lucas-Kanade algorithm. This approach can help to provide both greater accuracy, by locating features that may have moved distances greater than the current window size, while also improving efficiency by means of reducing the size of the search area within higher resolution versions of an image, by first localising the general target area within lower resolution versions that are less computationally expensive to search. 

Of particular interest from the work of Campbell \textit{et al} \cite{campbell} is an approach described for detecting environment hazards solely via the exploitation of the optical flow field. The idea proposed bases itself around the observed discontinuities caused to the optical flow field by scene objects that appear at a different observed depth to camera than the ``normal terrain". In the paper this behaviour is described as a violation of the ``planar world hypothesis", with objects closer to the camera than the ground causing a positive violation, and objects at a greater depth to the camera relative to the ground causing a negative violation. As a consequence, the authors discuss how owing to the effects of motion parallax, it is possible to observe clear differences between subsequent frames in the displacement of scene objects demonstrating either a positive, negative or no violation. This in turn maps onto discontinuities observed in the optical flow field, with objects that move significantly closer to the camera relative to the ground demonstrating longer optical flow vectors, and objects further away from the camera demonstrating the opposite. Given the use of such a seemingly `simple' metric as the proportional length of optical flow vectors, this approach appears to provide positive results, with the authors describing how in practice their system was able to ``detect precipices as near as 3cm". 

An alternative approach proposed by Low and Wyeth \cite{low-wyeth} involves combining sparse feature detection with appearance-based matching in order to track corresponding features between video frames. Firstly, initial features are detected using the same Shi and Tomasi corner detector \cite{shi-tomasi-good-features-to-track} as used in the approach devised by Campbell \textit{et al} \cite{low-wyeth}. 

%This detector is almost identical to the original Harris corner detector \cite{harris-corner} from which it is based, with the only main difference focussing on a change in the use of Eigenvalues to score and classify if an image region should be identified as a corner or not. This slight modification was demonstrated by Shi and Tomasi to provide a greater level of tracking stability and robustness in comparison to the original Harris corner detector \cite{shi-tomasi-good-features-to-track}, and as a result is typically preferred over the original detector proposed by Harris and Stephens. 

Once suitable features have been identified using corner detection, the authors then chose to utilise an appearance-based template matching technique to try and match corresponding windows of neighbouring pixels between subsequent images around each of the detected features. This contrasts with the work of Campbell \textit{et al} \cite{campbell} who instead adopt an entirely feature-based approach to matching corresponding portions of subsequent images. Using the template matching function provided by the \textit{OpenCV} library, the authors describe how a variety of appearance-based matching metrics could easy be evaluated via a simple modification to this function. The authors go on to report how they finally settled on the use of Normalised Cross Correlation as the matching metric, owing primarily to its improved robustness to lighting changes and simple score thresholding range of between 0 and 1, with 1 indicating a perfect match. 

Once obtained, optical flow information is converted to more useful range data using a method known as `Time-to-Contact'. With this approach, it is possible to represent distance to a target as a unit of time \cite{alenya}, and itself is based heavily upon biological mechanisms identified as responsible for providing the correct timing of actions and motion within the brain of humans and birds \cite{lee-young}. Other examples of the use of Time-to-Contact within vision-based systems involving obstacle detection are available in \cite{alenya}, \cite{sagrebin} and \cite{thomas}.

%Contrasting with the more ``ad-hoc" approach adopted by Campbell \textit{et al}, Low and Wyeth choose to generate a map that represents obstacle range information in order to support the detection of obstacles via the analysis of angular range output. 

A major challenge relating to the use of optical flow methods for obstacle detection, and one discussed in the work of both Low and Wyeth \cite{low-wyeth}, and Campbell \textit{et al} \cite{low-wyeth} is the effect that disturbances in rotation (deliberate or otherwise) can cause on the observed optical flow vectors of features. While both papers propose alternative methods for removing the effects of rotation (the use of an Inertial Measurement Unit gyroscope in the case of Low and Wyeth, and a calibrated cylindrical coordinate model in the case of Campbell \textit{et al}), both appear to agree that rotational movement should be accounted separately to translational movement in order to ensure accurate results are obtained from the optical flow field. 

Ulrich and Nourbakhsh \cite{ulrich-nourbakhsh} present an altogether different approach to vision-based obstacle detection. Rather than scrutinising differences in motion behaviour of scene objects, the authors instead focus on comparing differences in colour between the ground and other objects in the robot's field of view. Ulrich and Nourbakhsh argue that using colour as a detection metric for obstacles can prove to be less computationally expensive than ``range-based" obstacle detection methods like optical flow or stereo vision, and in certain cases, have also shown to be more accurate and reliable - particularly in cases involving the detection of small or flat objects close the ground or where a system needs to be able to differentiate between terrain surfaces. 

A key aspect from the Ulrich and Nourbakhsh paper provides details on how their system handles the effects caused by changes in illumination. In their solution to this well-known issue within computer vision, the authors discuss the use of an alternative colour space to RGB for representing input images that allows for the undesired effects caused by changes in illumination between subsequent frames to be negated. By converting images to use the `Hue-Saturation-Intensity' colour space, the actual colour of objects within the image (represented by the Hue and Saturation channels) become less sensitive to changes in brightness (represented by the Intensity channel). As well as providing greater resistance to illumination changes, using a colour space that separates colour from brightness also provided an easier platform from which the authors could further remove noise via channel thresholding \cite{ulrich-nourbakhsh}. 

In a survey conducted into investigating various methodologies of appearance-based template matching, Perveen \textit{et al} \cite{perveen} discuss an enhancement to traditional template matching methods that has been shown to support the successful detection of objects whose observed appearance has been altered between consecutive images as a result of change in orientation. 
 
 In their paper, Perveen \textit{et al} describe how ``Grayscale-based template matching" utilises multiple pyramid structures (used traditionally to provide support for scale invariance \cite{lowe}) in order to provide support for rotational-invariance within appearance-based features detectors. By generating a new pyramid structure to represent every possible orientation of the target pattern, once combined these structures can be used to significantly improve the chances of a pattern being relocated in the current image, even after potentially experiencing a significant rotational transformation \cite{perveen}. Additional work published in \cite{kim} discusses further improvements to the efficiency of this approach by removing pixels deemed unlikely to ever match a target pattern from further consideration, thereby reducing computational cost. 
 
  The development of vision-based systems invariant to object rotation has been an area of keen research interest and progress over recent years. While a number of solutions now exist for providing resilience to changes in the rotation of objects \cite{sift}, in the majority of cases these solutions are only applicable to detection methods based upon the use of feature descriptors for representing scene objects, rather than the use of an object's appearance. This is particularly important given the assumption that for most objects, even a slight rotation will cause its \textit{appearance} to change considerably in terms of pixel values. As such, a key weakness of appearance-based detectors appears to focus around failures to find matches of an object that has undergone a rotation, caused by the use of an explicit pattern representing the appearance of that object as the primary metric for similarity equality \cite{find-citation}.
 
In addition to the 
 
 


\subsection{Project Overview}

Making use of recent advances in camera technology with appropriate computer vision techniques, this project aims to perform a viability study into the use of appearance-based pattern matching and optical flow analysis to provide an estimation into the general ``shape" of the current terrain, while also supporting the detection of obstacles in the path of a mobile robot. 

Through this system, it should be possible to identify the presence of both positive, and negative obstacles (e.g. rocks and pits respectively), providing a reasonable indication of their general size and location.

In addition, it is predicted that such a system will also be able to provide an estimation of the speed and change in rotation/orientation of the robot as it traverses along a path. These will be calculated as by-products of the terrain inference mechanism, and could form part of a larger visual odometry system.

While the primary aims of the proposed project are research-focussed, the ultimate goal of the project will be to implement the system onto a working mobile robot, such as the `IDRIS' all-terrain wheeled robot currently in use by the Aberystwyth University Computer Science department.

\subsection{Motivation}



\section{Analysis}


\subsection{Overview}

\subsection{Research Aims}

\subsection{Design Decisions}

\subsubsection{Feature Matching}

\subsubsection{Development Environment}

\section{Research Methodology}

\subsection{Overview}

\subsection{Development Process}

\subsection{Testing}

\section{Project Working Hypothesis}



