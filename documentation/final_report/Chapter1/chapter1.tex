\chapter{Background \& Objectives}

%This section should discuss your preparation for the project, including background reading, your analysis of the problem and the process or method you have followed to help structure your work.  It is likely that you will reuse part of your outline project specification, but at this point in the project you should have more to talk about. 
%
%\textbf{Note}: 
%
%\begin{itemize}
%   \item All of the sections and text in this example are for illustration purposes. The main Chapters are a good starting point, but the content and actual sections that you include are likely to be different.
%   
%   \item Look at the document on the Structure of the Final Report for additional guidance. 
%   
%\end {itemize}
%
%\section{Background}
%What was your background preparation for the project? What similar systems or research techniques did you assess? What was your motivation and interest in this project? 
%
%\section{Analysis}
%Taking into account the problem and what you learned from the background work, what was your analysis of the problem? How did your analysis help to decompose the problem into the main tasks that you would undertake? Were there alternative approaches? Why did you choose one approach compared to the alternatives? %%There should be a clear statement of the research questions, which you will evaluate at the end of the work. %%In most cases, the agreed objectives or requirements will be the result of a compromise between what would ideally have been produced and what was felt to be possible in the time available. A discussion of the process of arriving at the final list is usually appropriate.
%
%\section{Research Method}
%You need to describe briefly the life cycle model or research method that you used. You do not need to write about all of the different process models that you are aware of. Focus on the process model or research method that you have used. It is possible that you needed to adapt an existing method to suit your project; clearly identify what you used and how you adapted it for your needs.

\section{Background}

The ability to choose between two or more alternate choices is one that comes innate in humans and is, in most circumstances, one that we rarely consciously observe. As humans, we are expected to make decisions both large and small as part of everyday life. While most decisions will have typically little impact requiring almost unnoticeable levels of thought (e.g. \textit{``do I have peas or beans tonight with dinner?"}), others that could have potentially life-changing consequences (e.g. \textit{"do I take this new job or not?"}) will force an individual to consider much more closely.

For a human faced with making such decisions, identifying which out of a number of possible choices constitutes the ``right" decision is often regarded as a challenging and highly subjective task. %Each decision that a person makes will vary based upon their individual characteristics, which as a consequence can often result in contrasting decisions being taken in response to the same situation.% 
Before settling on a final decision, an individual will typically identify and evaluate all possible choices by considering each upon a weighting of merit \cite{rational-decision-model}. While variations in weighting between specific individuals can cause different decision outcomes, an important realisation comes from the understanding that all considerations made by an individual rely on their \textit{trust} in the accuracy and truth of information gathered during the identification and comparison of these alternative choices.

 In considering any kind of autonomous system that is expected to make unattended decisions, a crucial yet not always obvious aspect is a reliance upon this idea of \textit{trust}. In the same way as observed in humans, a system will typically depend upon underlying information from which it can assess that the final decision when made is most appropriate for a given situation.
 
 This information may be obtained from a variety of sources, ranging from raw sensor data up to the output of a sophisticated high-level algorithm. While the type and source of information may vary wildly from system-to-system, the underlying notion that at the time of making the final decision this information can, in one form or another, be \textit{trusted} to provide a reliable and accurate representation of the actual situation remains exactly the same. 
 
Given the importance of the relationship between reliable information and correct decision making, a major challenge within the field of autonomous robotics is maximising both the accuracy of input data, and the robustness of systems used to verify such accuracy.

  Accurate and efficient navigation plays a crucial role in allowing robots that require autonomous motion capabilities to make safe, yet objective decisions on how best to traverse from a starting position to a target position. This idea becomes evermore important when it is considered that a key use case for robotic systems typically involves the undertaking of tasks within environments deemed unfeasible or unsafe for humans to complete themselves. Observing examples of tasks where autonomous robots are expected to operate in particularly harsh environments, including bomb disposal \cite{bomb-disposal} and planetary exploration \cite{planet-explore}, it becomes clear that the ability to perform safe and robust navigation is vital to the survival of both mission and device.

As a problem-space, autonomous navigation can be decomposed into two key areas of focus; one of \textit{reactive} navigation, and the other of \textit{deliberative} navigation. Reactive, or local navigation is concerned with controlling the path of the robot through the immediate surrounding area, focussing in particular on the safe traversal around potentially dangerous terrain hazards including obstacles, steep slopes and precipices. In contrast, deliberative or global navigation is tasked with planning the ``high-level" path that a robot will follow in order to reach its destination. As a consequence, deliberative navigation tends to adopt a greater level of focus on calculating the most \textit{optimal} path through the environment over necessarily the one that is ``safest" for the robot. Traditionally within autonomous navigation systems, feedback from both the reactive and deliberative components is combined in order to arrive at final path that balances both safety and objectivity \cite{mer-rover}. 

A vital contributor to the success of reactive navigation and to the overall survival of an autonomous robot is the identification and subsequent avoidance of obstacles. As a result, it is a keen and well-explored area of robotics research, with a variety of approaches now available adopting many types of sensor including sonar \cite{sonar-sensor} and laser scanning \cite{laser-sensor}. An alternative approach that has enjoyed increasingly greater research focus over the past decade is that of vision-based obstacle avoidance. This involves the analysis of images captured from one or more digital cameras in order to detect and classify possible dangers currently situated within the robot's field of view. 

Cameras are becoming an increasingly favourable sensor for use on robotic systems, due primarily to the impressive variety and quantity of potential data that can be captured \textit{simultaneously} from a single device. They are also one of few sensors that have experienced a consistent reduction in price over the past decade, making them viable for many types of project application and budget. 

\subsection{Related Work}

Many solutions to vision-based navigation and obstacle avoidance have been suggested and published, with the majority proposing new or improved approaches of combining standard computer-vision techniques (e.g. optical flow, feature tracking and patch tracking) with a variety of hardware configurations (e.g. monocular systems, stereo cameras, vision and 3D hybrid). Typically these either attempt to improve on previously published performance results, or focus on the avoidance of specific types of obstacle (e.g. precipice or animal detection). By investigating existing research conducted in this area, it is possible to identify and compare potential benefits and challenges between various approaches that will in turn feed into future design decisions.

The first approach to be noted is that from the work of J. Campbell \textit{et al.} \cite{j-campbell} in which the authors propose a system for estimating the change in position and rotation of a robot as it moves throughout its environment using information obtained solely via a single consumer-grade colour camera. The approach described involves the use of feature-based tracking to estimate the optical flow-field between subsequent video frames, before taking these optical flow vectors currently corresponding to image coordinates and back-projecting them into a ``robot-centred" coordinate system to establish the incremental ``real world" translation and rotation of the robot over a given time period. The detection and subsequent tracking of matching features between captured frames is provided via the \textit{OpenCV} library implementation of the Lucas-Kanade algorithm \cite{opencv-lucas-kanade-features}. The authors report that this implementation provides a more efficient and robust version of the original algorithm proposed by Lucas and Kanade \cite{lucas-kanade-features}. 

Upon further investigation this improvement appears to be attributed to the proposal published by J. Bouguet \cite{j-bouguet} to combine a multi-scale pyramidal representation of an image within the existing Lucas-Kanade algorithm. This approach can help to provide both greater accuracy, by locating features that may have moved distances greater than the current window size, while also improving efficiency through a reduction in the size of the search area within higher resolution versions of an image, by first localising the general target area within lower resolution versions. 

Of particular interest in the work of J. Campbell \textit{et al.} \cite{j-campbell} is the proposal of detecting environment hazards via the exploitation of the optical flow field. The idea discussed in the paper bases itself around the observed discontinuities caused to the optical flow field between the ``normal ground" and potential hazards. The authors found that positive objects that appear closer to the camera (e.g. rocks) demonstrate a greater displacement between subsequent frames than the ground, and negative obstacles (e.g. precipices) demonstrate a smaller displacement. 
   

\subsection{Motivation}

Combining recent advances in camera technology with appropriate computer vision algorithms and technique, the proposed project aims to design and implement a vision-based software application capable of estimating the general ``shape" of the terrain currently in front of a moving robot as it follows a route through its environment. Through this system, it should be possible to identify the presence of both positive, and negative obstacles (e.g. rocks and pits respectively), providing a reasonable indication of their general size and location. \\

In addition, it is predicted that such a system will also be able to provide an estimation of the speed and change in rotation/orientation of the robot as it traverses along a path. These will be calculated as by-products of the terrain inference mechanism, and could form part of a larger visual odometry system. \\

While the primary aims of the proposed project are research-focussed, the ultimate goal of the project will be to implement the system onto a working mobile robot, such as the `IDRIS' all-terrain wheeled robot currently in use by the Aberystwyth University Computer Science department. \\

\section{Analysis}


\subsection{Overview}


\subsection{Research Aims}


\subsection{Design Decisions}

\subsubsection{Feature Matching}

\subsubsection{Development Environment}

\subsubsection{Testing}

\subsubsection{External Libraries}


\section{Research Methodology}

\subsection{Research Type}

\subsection{Development Process}



