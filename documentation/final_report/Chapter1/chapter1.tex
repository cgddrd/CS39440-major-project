\chapter{Background \& Objectives}

%This section should discuss your preparation for the project, including background reading, your analysis of the problem and the process or method you have followed to help structure your work.  It is likely that you will reuse part of your outline project specification, but at this point in the project you should have more to talk about. 
%
%\textbf{Note}: 
%
%\begin{itemize}
%   \item All of the sections and text in this example are for illustration purposes. The main Chapters are a good starting point, but the content and actual sections that you include are likely to be different.
%   
%   \item Look at the document on the Structure of the Final Report for additional guidance. 
%   
%\end {itemize}
%
%\section{Background}
%What was your background preparation for the project? What similar systems or research techniques did you assess? What was your motivation and interest in this project? 
%
%\section{Analysis}
%Taking into account the problem and what you learned from the background work, what was your analysis of the problem? How did your analysis help to decompose the problem into the main tasks that you would undertake? Were there alternative approaches? Why did you choose one approach compared to the alternatives? %%There should be a clear statement of the research questions, which you will evaluate at the end of the work. %%In most cases, the agreed objectives or requirements will be the result of a compromise between what would ideally have been produced and what was felt to be possible in the time available. A discussion of the process of arriving at the final list is usually appropriate.
%
%\section{Research Method}
%You need to describe briefly the life cycle model or research method that you used. You do not need to write about all of the different process models that you are aware of. Focus on the process model or research method that you have used. It is possible that you needed to adapt an existing method to suit your project; clearly identify what you used and how you adapted it for your needs.

\section{Background}

\subsection{Introduction}

The ability to decide between one or more alternate choices is a complex skill that as humans, we possess but rarely consciously observe. As human beings, we are expected to make decisions both large and small as part of everyday life, but . While most decisions will have typically little impact requiring almost unnoticeable levels of thought (e.g. \textit{``do I have peas or beans tonight with dinner?"}), others that could have potentially life-changing consequences (e.g. \textit{"do I take this new job or not?"}) will force an individual to consider much more closely.

For a human faced with making such decisions, identifying which out of a number of possible choices constitutes the ``right" decision is often regarded as a challenging and highly subjective task. %Each decision that a person makes will vary based upon their individual characteristics, which as a consequence can often result in contrasting decisions being taken in response to the same situation.% 
Before settling on a final decision, an individual will typically identify and evaluate all possible choices by considering each upon a weighting of merit \cite{rational-decision-model}. While variations in weighting between specific individuals can cause different decision outcomes, an important realisation comes from the understanding that all considerations made by an individual rely on their \textit{trust} in the accuracy and truth of information gathered during the identification and comparison of these alternative choices.

 In considering any kind of autonomous system that is expected to make unattended decisions, a crucial yet not always obvious aspect is a reliance upon this idea of \textit{trust}. In the same way as observed in humans, a system will typically depend upon underlying information from which it can assess that the final decision when made is most appropriate for a given situation.
 
 This information may be obtained from a variety of sources, ranging from raw sensor data up to the output of a sophisticated high-level algorithm. While the type and source of information may vary wildly from system-to-system, the underlying notion that at the time of making the final decision this information can, in one form or another, be \textit{trusted} to provide a reliable and accurate representation of the actual situation remains exactly the same. 
 
Given the importance of the relationship between reliable information and correct decision making, a major challenge within the field of autonomous robotics is maximising both the accuracy of input data, and the robustness of systems used to verify such accuracy.

  Accurate and efficient navigation plays a crucial role in allowing robots that require autonomous motion capabilities to make safe, yet objective decisions on how best to traverse from a starting position to a target position. This idea becomes evermore important when it is considered that a key use case for robotic systems typically involves the undertaking of tasks within environments deemed unfeasible or unsafe for humans to complete themselves. Observing examples of tasks where autonomous robots are expected to operate in particularly harsh environments, including bomb disposal \cite{bomb-disposal} and planetary exploration \cite{planet-explore}, it becomes clear that the ability to perform safe and robust navigation is vital to the survival of both mission and device.

As a problem-space, autonomous navigation can be decomposed into two key areas of focus; one of \textit{reactive} navigation, and the other of \textit{deliberative} navigation. Reactive, or local navigation is concerned with controlling the path of the robot through the immediate surrounding area, focussing in particular on the safe traversal around potentially dangerous terrain hazards including obstacles, steep slopes and precipices. In contrast, deliberative or global navigation is tasked with planning the ``high-level" path that a robot will follow in order to reach its destination. As a consequence, deliberative navigation tends to adopt a greater level of focus on calculating the most \textit{optimal} path through the environment over necessarily the one that is ``safest" for the robot. Traditionally within autonomous navigation systems, feedback from both the reactive and deliberative components is combined in order to arrive at final path that balances both safety and objectivity \cite{mer-rover}. 

A vital contributor to the success of reactive navigation and therefore to the overall survival of an autonomous robot is the identification and subsequent avoidance of obstacles. As a result, the development of accurate and robust obstacle detection systems is a keen and well-explored area of robotics research, with a variety of approaches now available adopting many types of sensor including sonar \cite{sonar-sensor} and laser scanning \cite{laser-sensor}. An alternative approach that has enjoyed increasingly greater research focus over the past decade is that of vision-based obstacle avoidance. This involves the analysis of images captured from one or more digital cameras in order to detect and classify possible dangers currently situated within the robot's field of view. 

Cameras are becoming an increasingly favourable sensor for use on robotic systems, due primarily to the impressive variety and quantity of potential data that can be captured \textit{simultaneously} from a single device. They are also one of few sensors that have experienced a consistent reduction in price over the past decade \cite{campbell}, making them viable for many types of project application and budget. 

\subsection{Computer Vision Principles}

\subsection{Related Work}
\label{related-work}

The challenge of providing accurate and robust obstacle detection has, and remains to be major topic of focus within the field of computer vision. When beginning to investigate this area of research, it quickly becomes clear that there are an enormous variety of solutions already available. In the majority of cases, these solutions propose new or improved approaches to combining ``standard" computer-vision techniques with a variety of hardware configurations, with the aim of either improving on previously published results, or to focus on the avoidance of specific types of obstacle (e.g. precipice or animal detection). 

%From investigating existing research conducted in the area of visual-based navigation and obstacle avoidance, it is possible to identify and compare potential benefits and challenges between various approaches that will in turn feed into future design decisions.

\subsubsection{Feature-Based Methods}

One approach to be noted is that from the work of Campbell \textit{et al} \cite{campbell}, in which the authors propose a system for estimating the change in position and rotation of a robot using information obtained solely using a single consumer-grade colour camera. The approach describes the use of feature-based tracking to estimate the optical flow-field between subsequent video frames, before taking these optical flow vectors currently corresponding to image coordinates, and back-projecting them onto a ``robot-centred" coordinate system to establish the incremental `real world' translation and rotation of the robot over a given time period. The detection and subsequent tracking of matching features between captured frames is provided via the \textit{OpenCV} library implementation of the Lucas and Kanade algorithm \cite{opencv-lucas-kanade-features}. 

Once gathered, these correspondences between features (i.e. optical flow vectors) are filtered to help remove outliers caused by matching error or occlusion. The criteria used for this filtering process focusses on verifying a smooth straight-line motion of same features between subsequent frames. It is reported that concentrating on the movement history of features across a subset of previous frames can provide a robust defence against outliers. This is because by definition, an outlier would typically be expected to be found outside of the projected straight-line trajectory of the mis-matched feature, thereby causing the motion behaviour of this feature to suddenly become erratic. Through the use of this filtering technique in conjunction with the correction of perspective distortion via calibration, the authors report they are able to allow for a wider, and potentially lower quality range of features to be initially detected in order to ensure adequate coverage of the entire image scene. 

In the final stage of the process proposed in \cite{campbell}, the current frame is sectioned into regions representing the ``sky" and ``ground" individually, from within which the observed optical flow vectors are used to calculate an estimation of the robot's incremental rotation and translation respectively.

%The authors report that this implementation provides a more efficient and robust version of the original algorithm proposed by Lucas and Kanade \cite{lucas-kanade-features}. Upon further investigation this improvement appears to be attributed to the proposal published by Bouguet \cite{j-bouguet} in which the author combines a multi-scale pyramidal representation of an image with the existing Lucas-Kanade algorithm. This approach can help to provide both greater accuracy, by locating features that may have moved distances greater than the current window size, while also improving efficiency by means of reducing the size of the search area within higher resolution versions of an image, by first localising the general target area within lower resolution versions that are less computationally expensive to search. 

Of particular interest from the work of Campbell \textit{et al} \cite{campbell} is an approach described for detecting environment hazards solely via the exploitation of the optical flow field. The idea proposed bases itself around the observed discontinuities caused to the optical flow field by scene objects that appear at a different observed depth to camera than the ``normal terrain". In the paper this behaviour is described as a violation of the ``planar world hypothesis", with objects closer to the camera than the ground causing a positive violation, and objects at a greater depth to the camera relative to the ground causing a negative violation. As a consequence, the authors discuss how owing to the effects of motion parallax, it is possible to observe clear differences between subsequent frames in the displacement of scene objects demonstrating either a positive, negative or no violation. This in turn maps onto discontinuities observed in the optical flow field, with objects that move significantly closer to the camera relative to the ground demonstrating longer optical flow vectors, and objects further away from the camera demonstrating the opposite. Given the use of such a seemingly `simple' metric as the proportional length of optical flow vectors, this approach appears to provide positive results, with the authors describing how in practice their system was able to ``detect precipices as near as 3cm". 

An alternative approach proposed by Low and Wyeth \cite{low-wyeth} involved combining sparse feature detection with appearance-based matching in order to track corresponding features between video frames. Using the template matching algorithm provided by the \textit{OpenCV} library, the authors describe how a variety of appearance-based matching metrics could easy be evaluated via simple changes to function parameters. They report that they finally settled on the use of Normalised Cross Correlation as the matching metric, owing primarily to its improved robustness to lighting changes and simple score range falling between 0 and 1 (1 indicating a perfect match) that allowed for easy pixel thresholding.

%This detector is almost identical to the original Harris corner detector \cite{harris-corner} from which it is based, with the only main difference focussing on a change in the use of Eigenvalues to score and classify if an image region should be identified as a corner or not. This slight modification was demonstrated by Shi and Tomasi to provide a greater level of tracking stability and robustness in comparison to the original Harris corner detector \cite{shi-tomasi-good-features-to-track}, and as a result is typically preferred over the original detector proposed by Harris and Stephens. 

Once obtained, optical flow information was converted to more suitable range data using a method known as `Time-to-Contact'. Using this approach it is possible to represent distance to a known target as a unit of time \cite{alenya}, and is itself based heavily upon biological mechanisms identified as being responsible for providing the correct timing of actions and motion within the brain of humans and birds \cite{lee-young}. Other examples of the use of Time-to-Contact within vision-based systems providing obstacle detection include \cite{alenya}, \cite{sagrebin} and \cite{thomas}.

%Contrasting with the more ``ad-hoc" approach adopted by Campbell \textit{et al}, Low and Wyeth choose to generate a map that represents obstacle range information in order to support the detection of obstacles via the analysis of angular range output. 

A major challenge relating to the use of optical flow methods for obstacle detection, and one discussed in the work of both Low and Wyeth \cite{low-wyeth}, and Campbell \textit{et al} \cite{low-wyeth} is the effect that disturbances in rotation (deliberate or otherwise) can cause on the observed optical flow vectors of features. While both papers propose alternative methods for removing the effects of rotation (the use of an Inertial Measurement Unit gyroscope in the case of Low and Wyeth, and a calibrated cylindrical coordinate model in the case of Campbell \textit{et al}), both appear to agree that rotational movement should be accounted separately to translational movement in order to ensure accurate results are obtained from the optical flow field. 


\subsubsection{Appearance-Based Methods}

Ulrich and Nourbakhsh \cite{ulrich-nourbakhsh} present an altogether different approach to vision-based obstacle detection. As opposed to scrutinising differences in motion behaviour of scene objects, the authors instead focus on comparing differences in colour between the ground and other objects in the robot's field of view. Ulrich and Nourbakhsh argue that using colour as a detection metric for obstacles can prove to be less computationally expensive than ``range-based" obstacle detection methods like optical flow or stereo vision, and in certain cases, have also shown to be more accurate and reliable - particularly in cases involving the detection of small or flat objects close the ground or where a system needs to be able to differentiate between terrain surfaces. 

A key aspect from the Ulrich and Nourbakhsh paper discusses details of how their system is able to effectively handle image noise caused by changes in illumination. In their solution, the authors discuss the use of an alternative colour space to RGB for representing input images that allows for the undesired effects caused by changes in illumination between subsequent frames to be negated. By converting images to use the `Hue-Saturation-Intensity' colour space, the actual colour of objects within the image (represented by the Hue and Saturation channels) become less sensitive to changes in brightness (represented by the Intensity channel). As well as providing greater resistance to illumination changes, using a colour space that separates colour from brightness also provided an easier platform from which the authors could further remove noise via channel thresholding \cite{ulrich-nourbakhsh}. 

As part of a survey conducted into various methodologies of appearance-based template matching methods, Perveen \textit{et al} \cite{perveen} discuss an enhancement to traditional template matching that has been shown to support the successful detection of objects whose observed appearance has been altered as a result of changes to their orientation. 

  The development of vision-based systems that are invariant to object rotation has been an area of keen research interest and progress over recent years. While a number of solutions now exist for providing resilience to such changes \cite{sift}, the majority of these solutions are only applicable to detection methods that are based upon the use of feature descriptors for representing scene objects, rather than the use of an object's appearance. 
  
  This is particularly important given the assumption that for most objects, even a slight rotation will cause its \textit{appearance}, in terms of pixel values, to be modified considerably. As such, a key weakness of appearance-based detectors describes a failure to find correct matches of an object that has undergone a rotation. This is generally caused as a result of such detectors relying on an explicit pattern of object appearance for use as the primary metric when comparing image patches for similarity equality \cite{find-citation}.
 
 In their paper, Perveen \textit{et al} describe how ``Grayscale-based template matching" utilises multiple pyramid structures (used traditionally to provide support for scale invariance \cite{lowe}) to provide support for rotational-invariance within appearance-based features detectors. By generating a new pyramid structure to represent every possible orientation of the target pattern, these structures can, once combined, be used to significantly improve the chances of a pattern being relocated in the current image, even after potentially experiencing a significant rotational transformation \cite{perveen}. Additional work published in \cite{kim} discusses further improvements to the efficiency of this approach by removing pixels deemed unlikely to ever match a target pattern from further consideration, thereby reducing computational cost. 

Later in the same survey, Perveen \textit{et al} discuss an additional modification to traditional template matching methods that they describe as able to exploit an important observation from object-appearance behaviour in order to provide a dramatic reduction in the computation time required for processing image patches. This observation, and the potential consequences it can pose in terms of efficiency gains, is described succinctly by the authors themselves:

\indent \textit{``The shape of any object is defined solely by the shape of its edges. Therefore instead of matching of the whole pattern, we could extract its edges and match only the nearby pixels, thus avoiding some unnecessary computations. In common applications the achieved speed-up is usually significant."} \cite{perveen}

Further investigation has shown that a number of previous robotic projects have successfully utilised edge detection as a means of providing accurate obstacle detection \cite{hanumante} and \cite{borenstein}. Also of interest was the work conducted by Lin and Chunbo \cite{lin} in which they propose a solution into the issue of rotation invariance within appearance-based detection methods through combining of the use of contour images with the ``mean absolute difference" template matching method. 

\subsubsection{Hardware Configuration}
 
While many of the approaches mentioned thus far discuss the use of monocular-based systems (i.e a single camera) for their hardware configuration, many other examples of vision-based systems using alternative choices of hardware are also available. One of the most popular alternatives to monocular camera systems is a concept that can be easily described as `natural progression' in terms of technological evolution. This description is of course talking about the use of two or more cameras, formally known as stereo vision. 

Williamson \cite{williamson} proposes a trinocular stereo vision system capable of detecting small obstacles over great distances - \textit{``objects as small as 14cm high over ranges well in excess of 100m"}. He discusses how by using three cameras arranged in a triangular formation, his obstacle detection system is able to maintain high levels of reliability even in situations where an image demonstrates \textit{``texture in one direction, but not in the other"}. This reliability is particularly important when you consider that typical examples of this type of behaviour have been found to originate around pixels representing the edges of objects that subsequently such a system is then aiming to detect \cite{williamson}. 

Williamson also later states that through the use of more than one camera, his system is able to enjoy an improved level of general accuracy, achieved as a result of the significant level of additional information that can be gathered. Taking this statement solely at face value, it could be feasible to assume that the addition of further cameras may improve on this performance further still. On a contrary to this, Williamson warns that using anymore than three cameras will not make any significant difference to performance results, and in fact may actually impede results as a consequence of over-complication \cite{williamson}.

As already mentioned previously, Low and Wyeth \cite{low-wyeth} choose to combine the efforts of a monocular vision system, laser scanner and inertial measurement unit to provide highly accurate detection capabilities for a variety of obstacle types and sizes. A key advantage to adopting a hybridised hardware configuration comes from benefit that when presented with a situation where one system may fail for any given reason, one or both of the alternative systems is able to continue working. Assuming that appropriate choices in hardware were made during the design stages, multi-instrument setups can help to ensure that potentially critical information regarding the location of possible obstacles is not lost. 

An important consideration to realise however, is that by increasing the variety and number of hardware components, other crucial quantities relating to the design including items like power usage and space will also naturally need to be increased. When designing an obstacle detection and avoidance system for use on a robot designed to be small and lightweight, these additional overheads can prove to be less than ideal. 

\subsection{Project Overview}

Making use of recent advances in camera technology with appropriate computer vision techniques, this project focusses on conducting a viability study into the use of various feature matching techniques and optical flow analysis in order to provide an estimation into the general gradient of the current terrain, while also supporting the detection of nearby obstacles and an indication of the egomotion of a mobile robot. 

%Through this system, it should be possible to identify the presence of both positive, and negative obstacles (e.g. rocks and pits respectively), providing a reasonable indication of their general size and location. In addition, it is predicted that such a system will also be able to provide an estimation of the speed and change in orientation of the robot as it traverses along a path. These will be calculated as by-products of the terrain inference mechanism, and could be expected to eventually form part of a larger visual odometry and/or autonomous navigation system.

While this project is primarily research-focussed, an ultimate goal of the project would be to install such a system onto a working mobile robot. A proposed example of such a robot could be the `IDRIS' all-terrain, four-wheel-drive robot currently owned and operated by the Department of Computer Science at Aberystwyth University.

% FIGURE: IDRIS

\subsection{Motivation}

Motivation for undertaking this project comes primarily from a peaked interest into the field of computer vision and its associated topics gained following the decision to undertake a computer vision module at university.

The project also presented a key opportunity for the author to engage in a substantial piece of work falling outside of their prior experience, choosing to pursue a problem of a scientific or research-based nature, rather than one of `traditional' software engineering. 

Further opportunities were also presented for enhancing development skills using the C, C++ and Python programming languages, and in particular, the development of computer vision applications using popular image processing libraries.


\section{Analysis}

Focussing on conducting research as part of a scientific investigation, this project looks specifically into investigating the viability of the following proposed hypothesis: 

\indent \textit{``From images captured via a monocular camera system, analysis of extracted optical flow information can provide estimates into important terrain and hazard information useful in supporting the safe navigation of an autonomous robot as it makes its way through the current environment. In particular, these estimations can help to establish vital details regarding the general gradient condition of terrain; the location, size and shape of potential obstacles and the current speed and orientation of the robot itself."}
 
In the case of this project, such a statement is technically an example of a `working hypothesis'. Defined by the Collins English Dictionary as \textit{``a suggested explanation for a group of facts or phenomena, accepted as a basis for further verification"} \cite{collins}, a working hypothesis gives rise to the expectation for further investigative work to be conducted, even if this hypothesis is later discovered to fail \cite{century}. 

The history of work that led to the proposal of the above hypothesis actually began focussing not on obstacle detection, nor the inference of terrain gradients, but instead on investigating potential solutions towards a lightweight visual-based odometry system for installation aboard a mobile robot. 

It was not until the completion of initial background investigations into existing approaches (namely the work of Campbell \textit{et al.} \cite{campbell} - discussed further in Section \ref{related-work}) that key ideas surrounding the final hypothesis began to come into fruition. 

\subsection{Design of the Working Hypothesis} 

The hypothesis detailed in this report can be decomposed into four key areas of expectation. While all relate to the exploitation of observed optical flow behaviour, each focusses on providing an estimation into the status of a specific ``condition metric". 

While these will of course only be estimates, it is hoped that when brought together, these observations will provide a vital insight into the overall condition of the current terrain and robot egomotion. 

The four primary ``condition metrics" proposed under the hypothesis are as follows:

\begin{itemize}
	\item \textbf{Terrain gradient} - Provides an indication of how ``flat" the terrain surrounding the robot current is. Any significant changes to terrain slope (positive or negative) should be identified in order to allow any appropriate action (e.g. reduction in speed) to be taken.
	\item  \textbf{Oncoming obstacles} - The detection of both positive and negative obstacles (e.g. rocks and pits respectively) encountered along the path of a mobile robot as it moves through its environment. 
	\item \textbf{Robot speed} - Indication of a robot's speed based upon a prior knowledge of a fixed time interval and the distance moved by the robot within that time frame.
	\item \textbf{Robot orientation} - An indication of current orientation, based upon observations into changes in rotation made as a result of the robot turning. 
	
\end{itemize} 

The following subsections aim to provide a description and justification of each expectation, along with a discussion into the general ``thought processes" considered in order to arriving at a final proposal.

\subsubsection{Inference of Terrain Gradient}

A common observation of the background reading was a discussion around the notion of motion parallax, and in particular the potential applications it presented for extracting location and structure information of objects. 

The use of observed changes in motion between images to estimate the shape and location of 3D-structures is well-explored concept that has given rise to a plethora of innovate vision-based projects \cite{tiddeman}, \cite{snavely}.

\subsubsection{Obstacle Detection}

\subsubsection{Rotation Estimation}

\subsubsection{Speed Estimation}


\subsection{Task Planning}

%\subsection{Design Decisions}
%
%\subsubsection{Feature Matching}
%
%\subsubsection{Development Environment}



\section{Research Methodology}

\subsection{Overview}

\subsection{Development Process}

\subsection{Testing}




